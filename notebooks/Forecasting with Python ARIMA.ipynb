{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting with Python ARIMA\n",
    "\n",
    "This notebook explores and experiments with data forecasting models. The notebook is divided into sections with each section representing indicators present in the data set. Each section has the following structure:\n",
    "1. Data Wrangling: the data per indicator is split off from the imported dataset and made into a series.\n",
    "2. Data Exploration: the data is plotted using line plots, and seasonal decompose to visualise any trends and/or seasonality in the data.\n",
    "3. Statistical Testing: the data is tested to determine parameters to be passed into ARIMA/SARIMA(X) model as needed.\n",
    "4. Perseistence: a persistence model is used to forecast the data to make a bassline for expected performance.\n",
    "5. Parameter Testing: the data is used for forecasting with Auto ARIMA which searches a grid space for the best model.\n",
    "6. Model Testing and Evaluation: models and configuration with the best performance is used to forecast the data and evaluated with RMSE, MAPE and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset For Forecast - Sheet1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gloria\\Downloads\\Forecasting with Python ARIMA.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gloria/Downloads/Forecasting%20with%20Python%20ARIMA.ipynb#ch0000001?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdateutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparser\u001b[39;00m \u001b[39mimport\u001b[39;00m parse\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gloria/Downloads/Forecasting%20with%20Python%20ARIMA.ipynb#ch0000001?line=4'>5</a>\u001b[0m \u001b[39m# Importing the excel sheet and displaying information about the sheet.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gloria/Downloads/Forecasting%20with%20Python%20ARIMA.ipynb#ch0000001?line=5'>6</a>\u001b[0m imported_sheet \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mDataset For Forecast - Sheet1.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, parse_dates\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mPeriod\u001b[39;49m\u001b[39m'\u001b[39;49m], index_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPeriod\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gloria/Downloads/Forecasting%20with%20Python%20ARIMA.ipynb#ch0000001?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumns present in data: \u001b[39m\u001b[39m{\u001b[39;00mimported_sheet\u001b[39m.\u001b[39mcolumns\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gloria/Downloads/Forecasting%20with%20Python%20ARIMA.ipynb#ch0000001?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndicators present in data: \u001b[39m\u001b[39m{\u001b[39;00mimported_sheet[\u001b[39m'\u001b[39m\u001b[39mIndicator\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gloria\\PycharmProjects\\plotlyProject\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gloria\\PycharmProjects\\plotlyProject\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=664'>665</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=665'>666</a>\u001b[0m     dialect,\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=666'>667</a>\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=675'>676</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=676'>677</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=677'>678</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=679'>680</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Gloria\\PycharmProjects\\plotlyProject\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=571'>572</a>\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=573'>574</a>\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=574'>575</a>\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=576'>577</a>\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=577'>578</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Gloria\\PycharmProjects\\plotlyProject\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=929'>930</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=931'>932</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=932'>933</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Gloria\\PycharmProjects\\plotlyProject\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1212'>1213</a>\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1213'>1214</a>\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1214'>1215</a>\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1215'>1216</a>\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1216'>1217</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1217'>1218</a>\u001b[0m     f,\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1218'>1219</a>\u001b[0m     mode,\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1219'>1220</a>\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1220'>1221</a>\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1221'>1222</a>\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1222'>1223</a>\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1223'>1224</a>\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1224'>1225</a>\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1225'>1226</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1226'>1227</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/parsers/readers.py?line=1227'>1228</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Gloria\\PycharmProjects\\plotlyProject\\venv\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=783'>784</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=784'>785</a>\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=785'>786</a>\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=786'>787</a>\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=787'>788</a>\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=788'>789</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=789'>790</a>\u001b[0m             handle,\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=790'>791</a>\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=791'>792</a>\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=792'>793</a>\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=793'>794</a>\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=794'>795</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=795'>796</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=796'>797</a>\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Gloria/PycharmProjects/plotlyProject/venv/lib/site-packages/pandas/io/common.py?line=797'>798</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset For Forecast - Sheet1.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# Importing the excel sheet and displaying information about the sheet.\n",
    "imported_sheet = pd.read_csv(\"Dataset For Forecast - Sheet1.csv\", parse_dates=['Period'], index_col='Period')\n",
    "\n",
    "print(f\"Columns present in data: {imported_sheet.columns} \\n\")\n",
    "\n",
    "print(f\"Indicators present in data: {imported_sheet['Indicator'].unique()}\")\n",
    "\n",
    "print(imported_sheet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sheet_splitter(example_sheet, indicator):\n",
    "    \"\"\"\n",
    "    a function that checks if an indicator is present in an example sheet,\n",
    "    splits the example sheet based on the provided indicator and then,\n",
    "    outputs a CSV file and determines whether time series forecasts,\n",
    "    can be performed on the split data. Expects at least 15 data points,\n",
    "    to pass check\n",
    "\n",
    "    Args:\n",
    "    example_sheet-pandas dataframe\n",
    "    indicator-string\n",
    "\n",
    "    Return:\n",
    "    indicator_df-dataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Declare the indicator conditional\n",
    "    indicator_conditional = example_sheet['Indicator'] == indicator\n",
    "    \n",
    "    # Make of copy of a slice of the original dataframe\n",
    "    indicator_df = example_sheet[indicator_conditional].copy()\n",
    "    \n",
    "    # Determine how many data points are suitable for data forecasting\n",
    "    if (len(indicator_df) >= 15):\n",
    "        print(f\"Indicator : {indicator}, with length: {len(indicator_df)} can be forecast\")\n",
    "        return indicator_df\n",
    "        \n",
    "    else:\n",
    "        return f\"\"\"Indicator : {indicator} with length: {len(indicator_df)}, cannot be forecast, choose another indicator or check spelling\"\"\"\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "def persistence_model(series, split_ratio):\n",
    "    \"\"\"\n",
    "    A function that implements a walk forward persistence model and \n",
    "    evaluates it using RMSE.\n",
    "\n",
    "    Args:s\n",
    "    series- pandas time series\n",
    "    split_ratio- ratio of training set to test set- float(0.1-0.9)\n",
    "\n",
    "    Returns:\n",
    "    RMSE, MAPE, R2 Score \n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare data\n",
    "    X = series.values\n",
    "    years = series.index\n",
    "    X = X.astype('float32')\n",
    "    train_size = int(len(X) * split_ratio)\n",
    "    # check if series and split length can compute\n",
    "    if train_size > 0:\n",
    "        train, test = X[0:train_size], X[train_size:]\n",
    "        train_years, test_years = years[0:train_size], years[train_size:]\n",
    "\n",
    "        # walk-forward validation\n",
    "        history = [x for x in train]\n",
    "        predictions = list()\n",
    "        for i in range(len(test)):\n",
    "            # predict\n",
    "            yhat = history[-1]\n",
    "            predictions.append(yhat)\n",
    "            # observation\n",
    "            obs = test[i]\n",
    "            date = test_years[i]\n",
    "            history.append(obs)\n",
    "            print(f\"{date:%Y}: Predicted={yhat:.3f}, Expected={obs:.3f}\")\n",
    "\n",
    "        # report performance\n",
    "        rmse = sqrt(mean_squared_error(test, predictions))\n",
    "        mape = (mean_absolute_percentage_error(test, predictions)) * 100\n",
    "        r_2 = r2_score(test, predictions)\n",
    "        print(f'RMSE: {rmse:.3f}')\n",
    "        print(f'MAPE: {mape:.3f}%')\n",
    "        print(f'r2 SCORE: {r_2:.3f}')\n",
    "        return rmse, mape, r_2\n",
    "    else:\n",
    "        return \"\"\"train set not large enough, supply a longer series or increase split ratio\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_shift, dropnan=True):\n",
    "    \"\"\"\n",
    "    a function which takes a time series and makes different lags,\n",
    "    of the time series data. It then makes a dataframe containing,\n",
    "    the original data and it's lags\n",
    "\n",
    "    Args: \n",
    "    data-a time series\n",
    "    n_shift-the number of lags the data should be shifted by\n",
    "    dropnan-drop na values in the new dataframe, true by default\n",
    "\n",
    "    Return\n",
    "    final_supervised_data- DataFrame\n",
    "    \"\"\"\n",
    "    # holds imported data\n",
    "    df = pd.DataFrame(data)\n",
    "    # holds lags as added\n",
    "    cols = list()\n",
    "    # holds the column names for new lags added\n",
    "    names = list()\n",
    "\n",
    "    # a loop which calls the shift function a given number of times\n",
    "    for i in range(1, n_shift+1):\n",
    "        cols.append(df.shift(i))\n",
    "        names.append(f\"Value-{i}\")\n",
    "    \n",
    "    # the names for the created columns\n",
    "    names.append(\"Value\")\n",
    "    # concatenate all the lags together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    # concatenate imported data and lag list\n",
    "    final_supervised_data = pd.concat([agg,data], axis=1)\n",
    "    # rename columns\n",
    "    final_supervised_data.columns = names\n",
    "\n",
    "    # drop nan values if true\n",
    "    if dropnan:\n",
    "        final_supervised_data.dropna(inplace=True)\n",
    "\n",
    "    return final_supervised_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series, **kwargs):\n",
    "    \"\"\"\n",
    "    a function that performs the augmented Dick-fuller statistical test\n",
    "    to determine the unit root of a time series and the whether or not\n",
    "    the time series being tested is stationary.\n",
    "\n",
    "    Args:\n",
    "    series- time series\n",
    "    *kw- keyword arguments for the adfuller function\n",
    "    \"\"\"\n",
    "    # Store result of the adf test\n",
    "    result = adfuller(series, **kwargs)\n",
    "    \n",
    "    # Print results\n",
    "    print(f'ADF Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t{}: {}'.format(key, value))\n",
    "    print(f'Result: The series is {\"not \" if result[1] > 0.05 else \"\"}stationary')\n",
    "    return result[0], result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "from xgboost import XGBRegressor\n",
    "from numpy import asarray\n",
    "\n",
    "def xgboost_forecast(input_X, target_y, train_ratio=0.50, **kwargs):\n",
    "    \"\"\"\n",
    "    A function that executes a xgboost algorithm on\n",
    "    provided data.\n",
    "    :params input_X-the features for the model\n",
    "    :params target_y-the class/target variable\n",
    "    :params train-ratio-ratio of train to test, range float 0.1-0.9, default=0.50\n",
    "    :params kwargs- keyword arguments for xgboost\n",
    "\n",
    "    \"\"\"\n",
    "    # list to store predictions\n",
    "    predictions = list()\n",
    "    # years present in the dataframe\n",
    "    years = input_X.index\n",
    "    \n",
    "    # Change the values in the data to float type\n",
    "    X = input_X.values\n",
    "    input_X = X.astype('float32')\n",
    "\n",
    "    # Change the values in the data to float type\n",
    "    y = target_y.values\n",
    "    target_y = y.astype('float32')\n",
    "    \n",
    "    # split data into train and test\n",
    "    train_size = int(len(input_X) * train_ratio)\n",
    "    X_train, X_test = input_X[0:train_size], input_X[train_size:]\n",
    "    y_train, y_test = target_y[0:train_size], target_y[train_size:]\n",
    "    train_years, test_years = years[0:train_size], years[train_size:]\n",
    "    \n",
    "    # walk-forward validation\n",
    "    history = [x for x in X_train]\n",
    "    futures = [y for y in y_train]\n",
    "    \n",
    "    # a loop to make one step predictions\n",
    "    for i in range(len(y_test)):\n",
    "        # predict\n",
    "        model = XGBRegressor(objective='reg:squarederror', **kwargs)\n",
    "        model.fit(asarray(history), asarray(futures))\n",
    "        yhat = model.predict(X_test)\n",
    "        result = yhat[0]\n",
    "        predictions.append(result)\n",
    "        date = test_years[i]\n",
    "\n",
    "        # observation\n",
    "        obs = X_test[i]\n",
    "        history.append(obs)\n",
    "        futures.append(y_test[i])\n",
    "        print(f\"{date:%Y}: Predicted={result:.3f}, Expected={y_test[i]:.3f}\")\n",
    "\n",
    "    # report performance\n",
    "    rmse = sqrt(mean_squared_error(y_test, predictions))\n",
    "    mape = (mean_absolute_percentage_error(y_test, predictions)) * 100\n",
    "    r_2 = r2_score(y_test, predictions)\n",
    "    print(f'RMSE: {rmse:.3f}')\n",
    "    print(f'MAPE: {mape:.3f}%')\n",
    "    print(f'r2 SCORE: {r_2:.3f}')\n",
    "    \n",
    "    # plot performance\n",
    "    test_list = list(y_test)\n",
    "    results_df = pd.DataFrame(list(zip(test_list, predictions)), index=list(test_years),\n",
    "                              columns=['test', 'predictions'])\n",
    "    plt.plot(results_df['test'], label=\"Test Data\")\n",
    "    plt.plot(results_df['predictions'], color='red', label='Predictions')\n",
    "    plt.title('XGBoost Predictions V Test Data')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def linear_regression_forecast(input_X, target_y, train_ratio=0.50):\n",
    "    \"\"\"\n",
    "    a function that executes a linear regression model on provided data.\n",
    "    :params input_X-the features for the model\n",
    "    :params target_y-the class/target variable\n",
    "    :params train-ratio-ratio of train to test, range float 0.1-0.9, default=0.50\n",
    "    \n",
    "    \"\"\"\n",
    "    # list to make predictions\n",
    "    predictions = list()\n",
    "    # years present within the data\n",
    "    years = input_X.index\n",
    "    \n",
    "    # change values in data to float type\n",
    "    X = input_X.values\n",
    "    input_X = X.astype('float32')\n",
    "\n",
    "    # change values in data to float type\n",
    "    y = target_y.values\n",
    "    target_y = y.astype('float32')\n",
    "    \n",
    "    # split data into test and train sets\n",
    "    train_size = int(len(input_X) * train_ratio)\n",
    "    X_train, X_test = input_X[0:train_size], input_X[train_size:]\n",
    "    y_train, y_test = target_y[0:train_size], target_y[train_size:]\n",
    "    train_years, test_years = years[0:train_size], years[train_size:]\n",
    "\n",
    "    # walk-forward validation\n",
    "    history = [x for x in X_train]\n",
    "    futures = [y for y in y_train]\n",
    "    # predictions = list()\n",
    "    for i in range(len(y_test)):\n",
    "        # predict\n",
    "        model = LinearRegression()\n",
    "        model.fit(asarray(history), asarray(futures))\n",
    "        yhat = model.predict(X_test)\n",
    "        result = yhat[0]\n",
    "        predictions.append(result)\n",
    "        date = test_years[i]\n",
    "\n",
    "        # observation\n",
    "        obs = X_test[i]\n",
    "        history.append(obs)\n",
    "        futures.append(y_test[i])\n",
    "        print(f\"{date:%Y}: Predicted={result:.3f}, Expected={y_test[i]:.3f}\")\n",
    "    \n",
    "    # report performance\n",
    "    rmse = sqrt(mean_squared_error(y_test, predictions))\n",
    "    mape = (mean_absolute_percentage_error(y_test, predictions)) * 100\n",
    "    r_2 = r2_score(y_test, predictions)\n",
    "    print(f'RMSE: {rmse:.3f}')\n",
    "    print(f'MAPE: {mape:.3f}%')\n",
    "    print(f'R2 SCORE: {r_2:.3f}')\n",
    "\n",
    "    # plot performance\n",
    "    test_list = list(y_test)\n",
    "    results_df = pd.DataFrame(list(zip(test_list, predictions)), index=list(test_years),\n",
    "                              columns=['test', 'predictions'])\n",
    "    plt.plot(results_df['test'], label='Test Data')\n",
    "    plt.plot(results_df['predictions'], color='red', label='Predictions')\n",
    "    plt.title(\"Linear Regression Predictions V Test Data\")\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "\n",
    "def ARIMA_forecast(series, p, d, q, split_ratio=0.50):\n",
    "    \"\"\"\n",
    "    A function that makes a forecast using a ARIMA and evaluates \n",
    "    the model with RMSE.\n",
    "    params: series-pandas series\n",
    "    p: autoregressor term-int\n",
    "    d: differencing term-int\n",
    "    q: moving average term-int\n",
    "    split_ratio: training set and test set ratio-float(0.1-0.9)\n",
    "    \"\"\"\n",
    "    # make data into float type\n",
    "    X = series.values\n",
    "    X = X.astype('float32')\n",
    "    # save dates present within data\n",
    "    years = series.index\n",
    "\n",
    "    #create train and test sets\n",
    "    train_size = int(len(X) * split_ratio)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    train_years, test_years = years[0:train_size], years[train_size:]\n",
    "    \n",
    "    # walk-forward validation\n",
    "    history = [x for x in train]\n",
    "    predictions = list()\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        # predict\n",
    "        model = ARIMA(history, order=(p,d,q))\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        date = test_years[i]\n",
    "\n",
    "        # observation\n",
    "        obs = test[i]\n",
    "        history.append(obs)\n",
    "        print(f\"{date:%Y}: Predicted={yhat:.3f}, Expected={obs:.3f}\")\n",
    "\n",
    "    # report performance\n",
    "    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "    mape = (mean_absolute_percentage_error(test, predictions)) * 100\n",
    "    r_2 = r2_score(test, predictions)\n",
    "    print(f'RMSE: {rmse:.3f}')\n",
    "    print(f'MAPE: {mape:.3f}%')\n",
    "    print(f'R2_SCORE: {r_2:.3f}')\n",
    "\n",
    "    # plot performance\n",
    "    test_list = list(test)\n",
    "    results_df = pd.DataFrame(list(zip(test_list, predictions)), index=list(test_years),\n",
    "                              columns=['test', 'predictions'])\n",
    "    plt.plot(results_df['test'], label='Test Data')\n",
    "    plt.plot(results_df['predictions'], color='red', label='Predictions')\n",
    "    plt.title('ARIMA Predictions V Test Data')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adolescent Birth rate\n",
    "***\n",
    "This section deals with forecasting the Adolescent birth rate, there are 30 data points for this indicator. \n",
    "The data was split 70%-30% for training and testing respectively.\n",
    "\n",
    "\n",
    "### Results summary\n",
    "1. Data is trend stationary; that is, it has an implicit trend.\n",
    "2. ARIMA performs significantly better than the bassline.\n",
    "3. Data could be improved if it is gathered on a more granular level such as monthly or daily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split of the adolescent birth rate indicator rows from full dataset\n",
    "ABR_sheet = sheet_splitter(imported_sheet, \"Adolescent birth rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns present within Adolescent birth rate sheet\n",
    "ABR_sheet.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not meaningful for forecasting\n",
    "ABR_forecast = ABR_sheet.drop(['Indicator', 'State', 'LGA', 'Source'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot of Adolescent birth rates over time\n",
    "ABR_forecast.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the plot above, a very clear downward trend can be seen in the data, with one dip around 1992 and a bump around 2010. No seasonality is apparent from this plot, it also appears that there is very little noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required module\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Multiplicative Decomposition \n",
    "result_mul = seasonal_decompose(ABR_forecast['Value'], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(ABR_forecast['Value'], model='additive', extrapolate_trend='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot multiplicative decompose\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "fig.set_figwidth(11)\n",
    "fig.set_figheight(7)\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax[0,0].plot(result_mul.observed)\n",
    "ax[0,0].set(xlabel='year', ylabel='Births', title='Observed')\n",
    "\n",
    "ax[0,1].plot(result_mul.trend)\n",
    "ax[0,1].set(xlabel='year', ylabel='Births', title='Trend')\n",
    "\n",
    "ax[1,0].plot(result_mul.seasonal)\n",
    "ax[1,0].set(xlabel='year', ylabel='Range', title='Seasonal')\n",
    "\n",
    "ax[1,1].plot(result_mul.resid)\n",
    "ax[1,1].set(xlabel='year', ylabel='Range', title='Residuals')\n",
    "\n",
    "fig.suptitle('Multiplicative Decompose', fontweight =\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot additive decompose\n",
    "fig1, ax1 = plt.subplots(2, 2)\n",
    "\n",
    "fig1.set_figwidth(11)\n",
    "fig1.set_figheight(7)\n",
    "fig1.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax1[0,0].plot(result_add.observed)\n",
    "ax1[0,0].set(xlabel='year', ylabel='Births', title='Observed')\n",
    "\n",
    "ax1[0,1].plot(result_add.trend)\n",
    "ax1[0,1].set(xlabel='year', ylabel='Births', title='Trend')\n",
    "\n",
    "ax1[1,0].plot(result_add.seasonal)\n",
    "ax1[1,0].set(xlabel='year', ylabel='Range', title='Seasonal')\n",
    "\n",
    "ax1[1,1].plot(result_add.resid)\n",
    "ax1[1,1].set(xlabel='year', ylabel='Range', title='Residuals')\n",
    "\n",
    "fig1.suptitle('Additive Decompose', fontweight =\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "These two groups of plots above decompose the series, using multiplicative and additive methods. From the figures, it can be confirmed that there is no seasonality within the data. The decomposotion also shows us that most of the information in the series exists in the trends present within the Adolescent Birth Rate series. \n",
    "\n",
    "Another point to note is that both the additive and multiplicative decomposition of the series exposes the same information, and thus none has more explanatory power than the other. In the next cell we test for staionarity of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the series is stationary\n",
    "adf_test(ABR_forecast['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the data by one difference.\n",
    "ABR_forecast['1difference']=ABR_forecast['Value']-ABR_forecast['Value'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retest if the series is stationary\n",
    "adf_test(ABR_forecast['1difference'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the data by another difference\n",
    "ABR_forecast['2difference']=ABR_forecast['1difference']-ABR_forecast['1difference'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retest for stationarity\n",
    "adf_test(ABR_forecast['2difference'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Plotting autocorrelation and partial correlation graphs to determine p,d,q for ARIMA Model\n",
    "fig101=plot_acf(ABR_forecast['2difference'].dropna())\n",
    "fig102=plot_pacf(ABR_forecast['2difference'].dropna(), lags=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the Autocorrelation and Partial Correlation plots above, we can establish a range for the AR (p) and MA (q) terms. For MA we can use the autocorrelation plot to determine a range as to where the correlation between values becomes insignificant. In the plot above, this is after the zero mark, and such a good range would be 0-3 . For AR, we use the partial autocorrelation plot. In the plot we see that the correlation cuts off after lag-0, thus a good range would be from 0-3 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a Persistence Model on the Time series data\n",
    "persistence_model(ABR_forecast['Value'], 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "import warnings\n",
    "\n",
    "# Auto ARIMA to determine optimal Values for ARIMA parameters\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "stepwise_fit = auto_arima(ABR_forecast['Value'].iloc[0:21],\n",
    "                          start_p=0, start_q=0,\n",
    "                          max_p=3, max_q=3, m=12, \n",
    "                          start_P=0, seasonal=False,\n",
    "                          d=2, D=None, trace=True,\n",
    "                          error_action='ignore',\n",
    "                          suppress_warnings=True,\n",
    "                          stepwise=True)\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a model with the best parameters\n",
    "ARIMA_forecast(ABR_forecast['Value'], 0, 2, 0, split_ratio=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time series data to supervised ML data\n",
    "ABR_supervised = series_to_supervised(ABR_forecast['Value'], n_shift=2)\n",
    "print(ABR_supervised.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Xgboost algorithm\n",
    "xgboost_forecast(ABR_supervised.loc[:,[\"Value-1\", \"Value-2\"]], ABR_supervised[\"Value\"],\n",
    "                train_ratio=0.70, learning_rate=0.1, n_estimators=23, alpha=5, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Linear Regression algorithm\n",
    "linear_regression_forecast(ABR_supervised.loc[:,[\"Value-1\", \"Value-2\"]], ABR_supervised[\"Value\"],\n",
    "                           train_ratio=0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "For the Adolescent Birth Rate time series, the ARIMA model above performes significantly better than a persistence(baseline) model, with *RMSE of 0.243* as opposed to *1.795* for the bassline model. The ARIMA model was trained with 21 of 30 (70%) data points, this performance could be further improved if the data is gathered on more granular level asuch as monthly/ daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPT 3/ Penta 3 Coverage Rate\n",
    "***\n",
    "This section deals with forecasting the DPT 3 Coverage rate, there are 30 data points for this indicator. \n",
    "The data was split 70%-30% for training and testing respectively.\n",
    "\n",
    "\n",
    "### Results summary\n",
    "1. Data is possibly a random walk, and cannot be forecast with significant confidence.\n",
    "2. ARIMA performs same as the bassline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split of the DPT 3/Penta 3 coverage rate indicator rows from full dataset\n",
    "DPT3_df = sheet_splitter(imported_sheet, \"DPT 3/Penta 3 coverage rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns present within DPT 3/Penta 3 coverage rate sheet\n",
    "DPT3_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not meaningful for forecasting\n",
    "DPT3_forecast = DPT3_df.drop(['Indicator', 'State', 'LGA', 'Source'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot of DPT 3/Penta 3 coverage rate over time\n",
    "DPT3_forecast.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The plot has a U like trend, with the possibility of noisy data. The plot also looks jagged with large swings within the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicative Decomposition \n",
    "result_mul = seasonal_decompose(DPT3_forecast['Value'], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(DPT3_forecast['Value'], model='additive', extrapolate_trend='freq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting multiplicative Decomposition\n",
    "fig2, ax2 = plt.subplots(2, 2)\n",
    "\n",
    "fig2.set_figwidth(11)\n",
    "fig2.set_figheight(7)\n",
    "fig2.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax2[0,0].plot(result_mul.observed)\n",
    "ax2[0,0].set(xlabel='year', ylabel='Coverage rate', title='Observed')\n",
    "\n",
    "ax2[0,1].plot(result_mul.trend)\n",
    "ax2[0,1].set(xlabel='year', ylabel='Coverage rate', title='Trend')\n",
    "\n",
    "ax2[1,0].plot(result_mul.seasonal)\n",
    "ax2[1,0].set(xlabel='year', ylabel='Range', title='Seasonal')\n",
    "\n",
    "ax2[1,1].plot(result_mul.resid)\n",
    "ax2[1,1].set(xlabel='year', ylabel='Range', title='Residuals')\n",
    "\n",
    "fig2.suptitle('Multiplicative Decompose', fontweight =\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting Additive Decompose\n",
    "fig3, ax3 = plt.subplots(2, 2)\n",
    "\n",
    "fig3.set_figwidth(11)\n",
    "fig3.set_figheight(7)\n",
    "fig3.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax3[0,0].plot(result_add.observed)\n",
    "ax3[0,0].set(xlabel='year', ylabel='Coverage rate', title='Observed')\n",
    "\n",
    "ax3[0,1].plot(result_add.trend)\n",
    "ax3[0,1].set(xlabel='year', ylabel='Coverage rate', title='Trend')\n",
    "\n",
    "ax3[1,0].plot(result_add.seasonal)\n",
    "ax3[1,0].set(xlabel='year', ylabel='Range', title='Seasonal')\n",
    "\n",
    "ax3[1,1].plot(result_add.resid)\n",
    "ax3[1,1].set(xlabel='year', ylabel='Range', title='Residuals')\n",
    "\n",
    "fig3.suptitle('Additive Decompose', fontweight =\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the figures, it can be confirmed that there is no seasoanlity within the data. The decomposotion also shows us that most of the information in the series exists in the trends present within the DPT 3/Penta 3 coverage rate series. \n",
    "Another point to note is that both the additive and multiplicative decomposition of the series exposes the same information, and thus none has more explanatory power than the other. In the next cell we test for staionarity of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the data for stationarity\n",
    "adf_test(DPT3_forecast['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a difference of the data since it wasn't stationary\n",
    "DPT3_forecast['1difference']=DPT3_forecast['Value']-DPT3_forecast['Value'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retest the difference\n",
    "adf_test(DPT3_forecast['1difference'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot autocorelation and partial autocorrelation for the differenced series to determine,\n",
    "# value range for p,d,q arguments for ARIMA\n",
    "fig105=plot_acf(DPT3_forecast['1difference'].dropna())\n",
    "fig106=plot_pacf(DPT3_forecast['1difference'].dropna(), lags=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the Autocorrelation and Partial Correlation plots above, we can establish a range for the AR (p) and MA (q) terms. For MA we can use the autocorrelation plot to determine a range as to where the correlation between values becomes insignificant. In the plot above, this is after the zero mark, and such a good range would be 0-3 . For AR, we use the partial autocorrelation plot. In the plot we see that the correlation cuts off after lag-0, thus a good range would be from 0-3 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call and evaluate a persistence model to establish a baseline\n",
    "persistence_model(DPT3_forecast['Value'], 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the best fit ARIMA model given the range of parameters\n",
    "stepwise_fit = auto_arima(DPT3_forecast['Value'].iloc[0:21],\n",
    "                          start_p=0, start_q=0,\n",
    "                          max_p=3, max_q=3, m=12, \n",
    "                          start_P=0, seasonal=False,\n",
    "                          d=1, D=None, trace=True,\n",
    "                          error_action='ignore',\n",
    "                          suppress_warnings=True,\n",
    "                          stepwise=True)\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best ARIMA from search.\n",
    "ARIMA_forecast(DPT3_forecast['Value'], 0, 1, 0, split_ratio=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time series data to supervised ML data\n",
    "DPT3_supervised = series_to_supervised(DPT3_forecast['Value'], n_shift=2)\n",
    "print(DPT3_supervised.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Xgboost algorithm\n",
    "xgboost_forecast(DPT3_supervised.loc[:,[\"Value-1\", \"Value-2\"]], DPT3_supervised[\"Value\"],\n",
    "                train_ratio=0.70, learning_rate=0.3, n_estimators=90, alpha=10, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Linear Regression algorithm\n",
    "linear_regression_forecast(DPT3_supervised.loc[:,[\"Value-1\", \"Value-2\"]], DPT3_supervised[\"Value\"],\n",
    "                           train_ratio=0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "The ARIMA model which produces the best result does not have any AR and MA terms, only one term for differencing, this suggests the data might be a random walk. The best performing ARIMA model has the same performance as a persistence model, also suggests that the data is a random walk and the trained model doesn't show better explanatory power than a model based on predicting previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infant Mortality Rate\n",
    "***\n",
    "This section deals with forecasting the Infant Mortality rate, there are 30 data points for this indicator. \n",
    "The data was split 70%-30% for training and testing respectively.\n",
    "\n",
    "\n",
    "### Results summary\n",
    "1. Data is a smooth downward trend.\n",
    "2. ARIMA performs better than the bassline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split of the infant mortality rate indicator rows from full dataset\n",
    "IM_rate_df = sheet_splitter(imported_sheet, \"Infant Mortality rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns present within infant mortality rate sheet\n",
    "IM_rate_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not meaningful for forecasting\n",
    "IM_rate_forecast = IM_rate_df.drop(['Indicator', 'State', 'LGA', 'Source'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot of infant mortality over time\n",
    "IM_rate_forecast.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The plot has a smooth downward trend, with little or no noise in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicative Decomposition \n",
    "result_mul = seasonal_decompose(IM_rate_forecast['Value'], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(IM_rate_forecast['Value'], model='additive', extrapolate_trend='freq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Multiplicative Decomposition\n",
    "fig4, ax4 = plt.subplots(2, 2)\n",
    "\n",
    "fig4.set_figwidth(11)\n",
    "fig4.set_figheight(7)\n",
    "fig4.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax4[0,0].plot(result_mul.observed)\n",
    "ax4[0,0].set(xlabel='year', ylabel='IM rate', title='Observed')\n",
    "\n",
    "ax4[0,1].plot(result_mul.trend)\n",
    "ax4[0,1].set(xlabel='year', ylabel='IM rate', title='Trend')\n",
    "\n",
    "ax4[1,0].plot(result_mul.seasonal)\n",
    "ax4[1,0].set(xlabel='year', ylabel='Range', title='Seasonal')\n",
    "\n",
    "ax4[1,1].plot(result_mul.resid)\n",
    "ax4[1,1].set(xlabel='year', ylabel='Range', title='Residuals')\n",
    "\n",
    "fig4.suptitle('Multiplicative Decompose', fontweight =\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Decomposition\n",
    "fig5, ax5 = plt.subplots(2, 2)\n",
    "\n",
    "fig5.set_figwidth(11)\n",
    "fig5.set_figheight(7)\n",
    "fig5.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax5[0,0].plot(result_add.observed)\n",
    "ax5[0,0].set(xlabel='year', ylabel='IM Rate', title='Observed')\n",
    "\n",
    "ax5[0,1].plot(result_add.trend)\n",
    "ax5[0,1].set(xlabel='year', ylabel='IM Rate', title='Trend')\n",
    "\n",
    "ax5[1,0].plot(result_add.seasonal)\n",
    "ax5[1,0].set(xlabel='year', ylabel='Range', title='Seasonal')\n",
    "\n",
    "ax5[1,1].plot(result_add.resid)\n",
    "ax5[1,1].set(xlabel='year', ylabel='Range', title='Residuals')\n",
    "\n",
    "fig5.suptitle('Additive Decompose', fontweight =\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the figures, it can be confirmed that there is no seasoanlity within the data. The decomposotion also shows us that most of the information in the series exists in the trends present within the Infant Mortality rate series. \n",
    "Another point to note is that both the additive and multiplicative decomposition of the series exposes the same information, and thus none has more explanatory power than the other. In the next cell we test for staionarity of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the data for stationarity\n",
    "adf_test(IM_rate_forecast['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference the series \n",
    "IM_rate_forecast['1difference']=IM_rate_forecast['Value']-IM_rate_forecast['Value'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the difference\n",
    "adf_test(IM_rate_forecast['1difference'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the partial and autocorrelation plots for the difference of the time series\n",
    "fig111=plot_acf(IM_rate_forecast['1difference'].dropna())\n",
    "fig112=plot_pacf(IM_rate_forecast['1difference'].dropna(), lags=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the Autocorrelation and Partial Correlation plots above, we can establish a range for the AR (p) and MA (q) terms. For MA we can use the autocorrelation plot to determine a range as to where the correlation between values becomes insignificant. In the plot above, this is after the two lag, and such a good range would be 1-4 . For AR, we use the partial autocorrelation plot. In the plot we see that the correlation cuts off after lag-4, thus a good range would be from 2-5 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call and test a persistence model to establish a baseline\n",
    "persistence_model(IM_rate_forecast['Value'], 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the best ARIMA model for the series\n",
    "stepwise_fit = auto_arima(IM_rate_forecast['Value'].iloc[0:21],\n",
    "                          start_p=2, start_q=1,\n",
    "                          max_p=5, max_q=4, m=12, \n",
    "                          start_P=0, seasonal=False,\n",
    "                          d=1, D=None, trace=True,\n",
    "                          error_action='ignore',\n",
    "                          suppress_warnings=True,\n",
    "                          stepwise=True)\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best ARIMA model\n",
    "ARIMA_forecast(IM_rate_forecast['Value'], 3, 1, 1, split_ratio=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time series data to supervised ML data\n",
    "IM_rate_supervised = series_to_supervised(IM_rate_forecast['Value'], n_shift=2)\n",
    "print(IM_rate_supervised.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Xgboost algorithm\n",
    "xgboost_forecast(IM_rate_supervised.loc[:,[\"Value-1\", \"Value-2\"]], IM_rate_supervised[\"Value\"],\n",
    "                train_ratio=0.70, learning_rate=0.1, n_estimators=27, alpha=5, max_depth=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Linear Regression algorithm\n",
    "linear_regression_forecast(IM_rate_supervised.loc[:,[\"Value-1\", \"Value-2\"]], IM_rate_supervised[\"Value\"],\n",
    "                           train_ratio=0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results.\n",
    "The Time series for Infant mortality rate shows a smooth downward curve over time,and there is a significant difference in performance between the model *RMSE: 0.211* and the baseline *RMSE: 1.183* on the series. Assumption made on the data is that there is no seasonality in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skilled attendance at delivery or birth\n",
    "***\n",
    "The data for Skilled attendance at delivery or birth is insufficent for forecasting, because it has a short length of 8, with some years within the data are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split of the adolescent birth rate indicator rows from full dataset\n",
    "Skilled_birth_df = sheet_splitter(imported_sheet, \"Skilled attendance at delivery or birth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total fertility rate\n",
    "***\n",
    "This section deals with forecasting the Total fertility rate, there are 30 data points for this indicator. \n",
    "The data was split 70%-30% for training and testing respectively.\n",
    "\n",
    "\n",
    "### Results summary\n",
    "1. Data shows a smooth downward trend.\n",
    "2. ARIMA is better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split of the Total fertility rate indicator rows from full dataset\n",
    "TF_rate_df = sheet_splitter(imported_sheet, \"Total fertility rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns present within Total fertility rate sheet\n",
    "TF_rate_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not meaningful for forecasting\n",
    "TF_rate_forecast = TF_rate_df.drop(['Indicator', 'State', 'LGA', 'Source'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Line plot of Total fertility rateover time\n",
    "TF_rate_forecast.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The plot is shows a slow smooth downward trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicative Decomposition \n",
    "result_mul = seasonal_decompose(TF_rate_forecast['Value'], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(TF_rate_forecast['Value'], model='additive', extrapolate_trend='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot multiplicative decompose\n",
    "fig14, ax14 = plt.subplots(2, 2)\n",
    "\n",
    "fig14.set_figwidth(11)\n",
    "fig14.set_figheight(7)\n",
    "fig14.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax14[0,0].plot(result_mul.observed)\n",
    "ax14[0,0].set(xlabel='year', ylabel='T.Fertility rate', title='Observed')\n",
    "\n",
    "ax14[0,1].plot(result_mul.trend)\n",
    "ax14[0,1].set(xlabel='year', ylabel='T.Fertility rate', title='Trend')\n",
    "\n",
    "ax14[1,0].plot(result_mul.seasonal)\n",
    "ax14[1,0].set(xlabel='year', ylabel='Range', title='Seasonal')\n",
    "\n",
    "ax14[1,1].plot(result_mul.resid)\n",
    "ax14[1,1].set(xlabel='year', ylabel='Range', title='Residuals')\n",
    "\n",
    "fig14.suptitle('Multiplicative Decompose', fontweight =\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot additive decompose\n",
    "fig15, ax15 = plt.subplots(2, 2)\n",
    "\n",
    "fig15.set_figwidth(11)\n",
    "fig15.set_figheight(7)\n",
    "fig15.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax15[0,0].plot(result_add.observed)\n",
    "ax15[0,0].set(xlabel='year', ylabel='T.Fertility rate', title='Observed')\n",
    "\n",
    "ax15[0,1].plot(result_add.trend)\n",
    "ax15[0,1].set(xlabel='year', ylabel='T.Fertility rate', title='Trend')\n",
    "\n",
    "ax15[1,0].plot(result_add.seasonal)\n",
    "ax15[1,0].set(xlabel='year', ylabel='Range', title='Seasonal')\n",
    "\n",
    "ax15[1,1].plot(result_add.resid)\n",
    "ax15[1,1].set(xlabel='year', ylabel='Range', title='Residuals')\n",
    "\n",
    "fig15.suptitle('Additive Decompose', fontweight =\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the figures, it can be confirmed that there is no seasoanlity within the data. The decomposotion also shows us that most of the information in the series exists in the trends present within the Total fertility rate series. \n",
    "Another point to note is that both the additive and multiplicative decomposition of the series exposes the same information, and thus none has more explanatory power than the other. In the next cell we test for staionarity of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for stationarity\n",
    "adf_test(TF_rate_forecast['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a difference of the data\n",
    "TF_rate_forecast['1difference']=TF_rate_forecast['Value']-TF_rate_forecast['Value'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-test for stationarity\n",
    "adf_test(TF_rate_forecast['1difference'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot partial and auto correlation for the difference of total fertility rate\n",
    "fig125=plot_acf(TF_rate_forecast['1difference'].dropna())\n",
    "fig126=plot_pacf(TF_rate_forecast['1difference'].dropna(), lags=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the Autocorrelation and Partial Correlation plots above, we can establish a range for the AR (p) and MA (q) terms. \n",
    "For MA we can use the autocorrelation plot to determine a range as to where the correlation between values becomes insignificant. In the plot above, this is after the two mark, and such a good range would be 1-4 . For AR, we use the partial autocorrelation plot. In the plot we see that the correlation cuts off after lag-2, thus a good range would be from 1-4 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a persistence model \n",
    "persistence_model(TF_rate_forecast['Value'], 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for best parameters for ARIMA model\n",
    "stepwise_fit = auto_arima(TF_rate_forecast['Value'].iloc[0:21],\n",
    "                          start_p=1, start_q=1,\n",
    "                          max_p=3, max_q=3, m=12, \n",
    "                          start_P=0, seasonal=False,\n",
    "                          d=1, D=None, trace=True,\n",
    "                          error_action='ignore',\n",
    "                          suppress_warnings=True,\n",
    "                          stepwise=True)\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best parameters on the data\n",
    "ARIMA_forecast(TF_rate_forecast['Value'], 2, 1, 0, split_ratio=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time series data to supervised ML data\n",
    "TF_rate_supervised = series_to_supervised(TF_rate_forecast['Value'], n_shift=2)\n",
    "print(TF_rate_supervised.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Xgboost algorithm\n",
    "xgboost_forecast(TF_rate_supervised.loc[:,[\"Value-1\", \"Value-2\"]], TF_rate_supervised[\"Value\"],\n",
    "                train_ratio=0.70, learning_rate=0.1, n_estimators=31, alpha=5, max_depth=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Linear Regression algorithm\n",
    "linear_regression_forecast(TF_rate_supervised.loc[:,[\"Value-1\", \"Value-2\"]], TF_rate_supervised[\"Value\"],\n",
    "                           train_ratio=0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "On the Total Fertility time series, the data shows a smooth downward trend. The ARIMA model with *RMSE of *0.007* performs better than the baseline model *RMSE: 0.059* which is significant at 0.05 p-value. There might be seasonality within the data because of the shape of the partial and autocorrelation plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under 5 Mortality rate\n",
    "***\n",
    "This section deals with forecasting the Under 5 Mortality rate, there are 30 data points for this indicator. \n",
    "The data was split 70%-30% for training and testing respectively.\n",
    "\n",
    "\n",
    "### Results summary\n",
    "1. Data shows a smooth downward trend.\n",
    "2. ARIMA is better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split of the Under 5 Mortality rate indicator rows from full dataset\n",
    "U5_Mortality_rate_df = sheet_splitter(imported_sheet, \"Under 5 Mortality rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns present within Under 5 Mortality rate sheet\n",
    "U5_Mortality_rate_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not meaningful for forecasting\n",
    "U5_mortality_rate_forecast = U5_Mortality_rate_df.drop(['Indicator', 'State', 'LGA', 'Source'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Line plot of Under 5 Mortality rate over time\n",
    "U5_mortality_rate_forecast.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The plot is shows a smooth downward trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicative Decomposition \n",
    "result_mul = seasonal_decompose(U5_mortality_rate_forecast['Value'], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(U5_mortality_rate_forecast['Value'], model='additive', extrapolate_trend='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiplicative decompose\n",
    "fig16, ax16 = plt.subplots(2, 2)\n",
    "\n",
    "fig16.set_figwidth(11)\n",
    "fig16.set_figheight(7)\n",
    "fig16.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax16[0,0].plot(result_mul.observed)\n",
    "ax16[0,0].set(xlabel='year', ylabel='U5 mortality rate', title='Observed')\n",
    "\n",
    "ax16[0,1].plot(result_mul.trend)\n",
    "ax16[0,1].set(xlabel='year', ylabel='U5 mortality rate', title='Trend')\n",
    "\n",
    "ax16[1,0].plot(result_mul.seasonal)\n",
    "ax16[1,0].set(xlabel='year', ylabel='Range', title='Seasonal')\n",
    "\n",
    "ax16[1,1].plot(result_mul.resid)\n",
    "ax16[1,1].set(xlabel='year', ylabel='Range', title='Residuals')\n",
    "\n",
    "fig16.suptitle('Multiplicative Decompose', fontweight =\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot additive decompose \n",
    "fig17, ax17 = plt.subplots(2, 2)\n",
    "\n",
    "fig17.set_figwidth(11)\n",
    "fig17.set_figheight(7)\n",
    "fig17.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax17[0,0].plot(result_add.observed)\n",
    "ax17[0,0].set(xlabel='year', ylabel='U5 mortality rate', title='Observed')\n",
    "\n",
    "ax17[0,1].plot(result_add.trend)\n",
    "ax17[0,1].set(xlabel='year', ylabel='U5 mortality rate', title='Trend')\n",
    "\n",
    "ax17[1,0].plot(result_add.seasonal)\n",
    "ax17[1,0].set(xlabel='year', ylabel='Range', title='Seasonal')\n",
    "\n",
    "ax17[1,1].plot(result_add.resid)\n",
    "ax17[1,1].set(xlabel='year', ylabel='Range', title='Residuals')\n",
    "\n",
    "fig17.suptitle('Additive Decompose', fontweight =\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the figures, it can be confirmed that there is no seasoanlity within the data. The decomposotion also shows us that most of the information in the series exists in the trends present within the Under 5 Mortality rate series. \n",
    "Another point to note is that both the additive and multiplicative decomposition of the series exposes the same information, and thus none has more explanatory power than the other. In the next cell we test for staionarity of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for stationarity\n",
    "adf_test(U5_mortality_rate_forecast['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a difference of the data\n",
    "U5_mortality_rate_forecast['1difference']=U5_mortality_rate_forecast['Value']-U5_mortality_rate_forecast['Value'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the difference for stationarity\n",
    "adf_test(U5_mortality_rate_forecast['1difference'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot partial and autocorrelation plots for U5 mortality rate\n",
    "fig129=plot_acf(U5_mortality_rate_forecast['1difference'].dropna())\n",
    "fig130=plot_pacf(U5_mortality_rate_forecast['1difference'].dropna(), lags=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From the Autocorrelation and Partial Correlation plots above, we can establish a range for the AR (p) and MA (q) terms. For MA we can use the autocorrelation plot to determine a range as to where the correlation between values becomes insignificant. In the plot above, this is after the two mark, and such a good range would be 1-4 . For AR, we use the partial autocorrelation plot. In the plot we see that the correlation cuts off after lag-4, thus a good range would be from 2-5 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate persistence (baseline) model\n",
    "persistence_model(U5_mortality_rate_forecast['Value'], 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal parameters for ARIMA model\n",
    "stepwise_fit = auto_arima(U5_mortality_rate_forecast['Value'].iloc[0:21],\n",
    "                          start_p=2, start_q=1,\n",
    "                          max_p=5, max_q=4, m=12, \n",
    "                          start_P=0, seasonal=False,\n",
    "                          d=1, D=None, trace=True,\n",
    "                          error_action='ignore',\n",
    "                          suppress_warnings=True,\n",
    "                          stepwise=True)\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best parameters on data\n",
    "ARIMA_forecast(U5_mortality_rate_forecast['Value'], 4, 1, 1, split_ratio=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time series data to supervised ML data\n",
    "U5_mortality_rate_supervised = series_to_supervised(U5_mortality_rate_forecast['Value'], n_shift=2)\n",
    "print(U5_mortality_rate_supervised.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Xgboost algorithm\n",
    "xgboost_forecast(U5_mortality_rate_supervised.loc[:,[\"Value-1\", \"Value-2\"]], U5_mortality_rate_supervised[\"Value\"],\n",
    "                train_ratio=0.70, learning_rate=0.1, n_estimators=25, alpha=5, max_depth=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Linear Regression algorithm\n",
    "linear_regression_forecast(U5_mortality_rate_supervised.loc[:,[\"Value-1\", \"Value-2\"]], U5_mortality_rate_supervised[\"Value\"],\n",
    "                           train_ratio=0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "On the Total Fertility time series, the data shows a smooth downward trend. The ARIMA model with *RMSE of *0.252* performs significantly better than the baseline model *RMSE: 2.140*. There might be seasonality within the data because of the shape of the partial and autocorrelation plots."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9ead8b22896542875c54b7e11cc10bf18a6cbba30721984237252e598d6c499"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
